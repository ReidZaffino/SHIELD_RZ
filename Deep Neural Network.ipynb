{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Neural Network Design (for adversarial training), with Self Scaling Formulation:\n",
    "\n",
    "Reid Zaffino (zaffino@uwindsor.ca) 2021-09\n",
    "\n",
    "Resources from Coursera \"Neural Networks and Deep Learning\" Course\n",
    "\n",
    "Self Scaling Methodology from: Djahanshahi, Hormoz., \"A robust hybrid VLSI neural network architecture for a smart optical sensor.\" (1999). Electronic Theses and Dissertations. 737. https://scholar.uwindsor.ca/etd/737"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from six.moves import cPickle \n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path =  os.path.realpath('./')\n",
    "file1 = dir_path + \"\\cifar-10-batches-py\\data_batch_1\"\n",
    "\n",
    "f = open(file1, 'rb')\n",
    "datadict = cPickle.load(f,encoding='latin1')\n",
    "f.close()\n",
    "X = datadict[\"data\"] \n",
    "Y = datadict['labels']\n",
    "\n",
    "nonbin = []\n",
    "for i in range (len(Y)):\n",
    "    if (Y[i] != 0 and Y[i] != 1):\n",
    "        nonbin.append(i)\n",
    "\n",
    "X = np.delete(X, nonbin, axis = 0)\n",
    "Y = np.delete(Y, nonbin)\n",
    "#a_list = list(range(1, 1000))\n",
    "#X = np.delete(X, a_list, axis = 0)\n",
    "#Y = np.delete(Y, a_list)\n",
    "\n",
    "X = (X.reshape(X.shape[0], -1)/255.).T\n",
    "Y = Y.reshape(1, Y.shape[0])\n",
    "\n",
    "train_x = X\n",
    "train_y = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [3072, 20, 7, 5, 1] #  4-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        #(≈ 2 lines of code)\n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \n",
    "    return np.divide(1, 1 + np.exp(-Z)), Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(Z):\n",
    "    \n",
    "    #return Z * (Z > 0), Z\n",
    "    return np.tanh(Z), Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    #(≈ 1 line of code)\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    Z = np.dot(W,A) + b\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation, ss):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"tanh\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        #(≈ 2 lines of code)\n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        if(ss):\n",
    "            Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "            A, activation_cache = sigmoid(Z/len(A_prev))\n",
    "        \n",
    "        else:\n",
    "            Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "            A, activation_cache = sigmoid(Z)\n",
    "            \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    elif activation == \"tanh\":\n",
    "        #(≈ 2 lines of code)\n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        if(ss):\n",
    "            Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "            A, activation_cache = tanh(Z/len(A_prev))\n",
    "        \n",
    "        else:\n",
    "            Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "            A, activation_cache = tanh(Z)\n",
    "            \n",
    "        # YOUR CODE ENDS HERE\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters, ss):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->tanh]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- activation value from the output (last) layer\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> tanh]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    # The for loop starts at 1 because layer 0 is the input\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        #(≈ 2 lines of code)\n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"tanh\", ss)\n",
    "        caches.append(cache)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    #(≈ 2 lines of code)\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\", ss)\n",
    "    caches.append(cache)\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "          \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    # (≈ 1 lines of code)\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    cost = (-1/m)*np.sum(np.multiply(np.log(AL), Y) + np.multiply(np.log(1 - AL), 1 - Y))\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    dW = (1/m)*np.dot(dZ, A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, activation_cache):\n",
    "    \n",
    "    return np.multiply(dA, np.multiply(np.divide(1, 1 + np.exp(-activation_cache)), 1 - np.divide(1, 1 + np.exp(-activation_cache))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_backward(dA, activation_cache):\n",
    "    \n",
    "    #return np.multiply(dA, (activation_cache > 0) * 1)\n",
    "    return np.multiply(dA, 1 - np.tanh(activation_cache)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"tanh\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"tanh\":\n",
    "        #(≈ 2 lines of code)\n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        dZ =  tanh_backward(dA, activation_cache)\n",
    "        #print(activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        #(≈ 2 lines of code)\n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->tanh] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"tanh\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    #print(\"This stuff: \")\n",
    "    #print(Y.shape)\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    #print(\"This stuff 2.0: \")\n",
    "    #print(Y.shape)\n",
    "\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    #(1 line of code)\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    #(approx. 5 lines)\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "    grads[\"dW\" + str(L)] = dW_temp\n",
    "    grads[\"db\" + str(L)] = db_temp\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (tanh -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        #(approx. 5 lines)\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dA_prev_temp, current_cache, \"tanh\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    params -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    parameters = params.copy()\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    #(≈ 2 lines of code)\n",
    "    for l in range(L):\n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.5, num_iterations = 3000, print_cost=False, ss=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->tanh]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    #(≈ 1 line of code)\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> tanh]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        #(≈ 1 line of code)\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        AL, caches = L_model_forward(X, parameters, ss)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Compute cost.\n",
    "        #(≈ 1 line of code)\n",
    "    \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        cost = compute_cost(AL, Y)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "        # Backward propagation.\n",
    "        #(≈ 1 line of code)\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    " \n",
    "        # Update parameters.\n",
    "        #(≈ 1 line of code)\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "                \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6931471805601178\n",
      "Cost after iteration 100: 0.6930252636961056\n",
      "Cost after iteration 200: 0.6930244925332523\n",
      "Cost after iteration 300: 0.6930244876532351\n",
      "Cost after iteration 400: 0.6930244876223416\n",
      "Cost after iteration 500: 0.6930244876221351\n",
      "Cost after iteration 600: 0.6930244876221229\n",
      "Cost after iteration 700: 0.693024487622112\n",
      "Cost after iteration 800: 0.693024487622101\n",
      "Cost after iteration 900: 0.6930244876220899\n",
      "Cost after iteration 1000: 0.6930244876220789\n",
      "Cost after iteration 1100: 0.6930244876220681\n",
      "Cost after iteration 1200: 0.693024487622057\n",
      "Cost after iteration 1300: 0.693024487622046\n",
      "Cost after iteration 1400: 0.6930244876220352\n",
      "Cost after iteration 1500: 0.6930244876220241\n",
      "Cost after iteration 1600: 0.6930244876220132\n",
      "Cost after iteration 1700: 0.6930244876220022\n",
      "Cost after iteration 1800: 0.6930244876219912\n",
      "Cost after iteration 1900: 0.6930244876219803\n",
      "Cost after iteration 2000: 0.6930244876219693\n",
      "Cost after iteration 2100: 0.6930244876219582\n",
      "Cost after iteration 2200: 0.6930244876219473\n",
      "Cost after iteration 2300: 0.6930244876219365\n",
      "Cost after iteration 2400: 0.6930244876219254\n",
      "Cost after iteration 2500: 0.6930244876219144\n",
      "Cost after iteration 2600: 0.6930244876219034\n",
      "Cost after iteration 2700: 0.6930244876218924\n",
      "Cost after iteration 2800: 0.6930244876218815\n",
      "Cost after iteration 2900: 0.6930244876218705\n",
      "Cost after iteration 3000: 0.6930244876218594\n",
      "Cost after iteration 3100: 0.6930244876218485\n",
      "Cost after iteration 3200: 0.6930244876218377\n",
      "Cost after iteration 3300: 0.6930244876218266\n",
      "Cost after iteration 3400: 0.6930244876218157\n",
      "Cost after iteration 3500: 0.6930244876218048\n",
      "Cost after iteration 3600: 0.6930244876217938\n",
      "Cost after iteration 3700: 0.6930244876217827\n",
      "Cost after iteration 3800: 0.6930244876217718\n",
      "Cost after iteration 3900: 0.6930244876217609\n",
      "Cost after iteration 4000: 0.6930244876217497\n",
      "Cost after iteration 4100: 0.6930244876217388\n",
      "Cost after iteration 4200: 0.6930244876217279\n",
      "Cost after iteration 4300: 0.6930244876217169\n",
      "Cost after iteration 4400: 0.6930244876217059\n",
      "Cost after iteration 4500: 0.693024487621695\n",
      "Cost after iteration 4600: 0.6930244876216839\n",
      "Cost after iteration 4700: 0.6930244876216729\n",
      "Cost after iteration 4800: 0.693024487621662\n",
      "Cost after iteration 4900: 0.6930244876216509\n",
      "Cost after iteration 5000: 0.6930244876216399\n",
      "Cost after iteration 5100: 0.693024487621629\n",
      "Cost after iteration 5200: 0.693024487621618\n",
      "Cost after iteration 5300: 0.6930244876216071\n",
      "Cost after iteration 5400: 0.6930244876215962\n",
      "Cost after iteration 5500: 0.6930244876215851\n",
      "Cost after iteration 5600: 0.6930244876215741\n",
      "Cost after iteration 5700: 0.6930244876215631\n",
      "Cost after iteration 5800: 0.693024487621552\n",
      "Cost after iteration 5900: 0.693024487621541\n",
      "Cost after iteration 6000: 0.6930244876215301\n",
      "Cost after iteration 6100: 0.6930244876215191\n",
      "Cost after iteration 6200: 0.693024487621508\n",
      "Cost after iteration 6300: 0.693024487621497\n",
      "Cost after iteration 6400: 0.693024487621486\n",
      "Cost after iteration 6500: 0.6930244876214751\n",
      "Cost after iteration 6600: 0.6930244876214641\n",
      "Cost after iteration 6700: 0.6930244876214532\n",
      "Cost after iteration 6800: 0.6930244876214422\n",
      "Cost after iteration 6900: 0.6930244876214311\n",
      "Cost after iteration 7000: 0.6930244876214201\n",
      "Cost after iteration 7100: 0.6930244876214091\n",
      "Cost after iteration 7200: 0.693024487621398\n",
      "Cost after iteration 7300: 0.693024487621387\n",
      "Cost after iteration 7400: 0.693024487621376\n",
      "Cost after iteration 7500: 0.6930244876213649\n",
      "Cost after iteration 7600: 0.6930244876213539\n",
      "Cost after iteration 7700: 0.6930244876213429\n",
      "Cost after iteration 7800: 0.693024487621332\n",
      "Cost after iteration 7900: 0.6930244876213211\n",
      "Cost after iteration 8000: 0.69302448762131\n",
      "Cost after iteration 8100: 0.693024487621299\n",
      "Cost after iteration 8200: 0.6930244876212879\n",
      "Cost after iteration 8300: 0.6930244876212769\n",
      "Cost after iteration 8400: 0.6930244876212659\n",
      "Cost after iteration 8500: 0.6930244876212549\n",
      "Cost after iteration 8600: 0.6930244876212439\n",
      "Cost after iteration 8700: 0.6930244876212327\n",
      "Cost after iteration 8800: 0.6930244876212217\n",
      "Cost after iteration 8900: 0.6930244876212108\n",
      "Cost after iteration 9000: 0.6930244876211997\n",
      "Cost after iteration 9100: 0.6930244876211886\n",
      "Cost after iteration 9200: 0.6930244876211777\n",
      "Cost after iteration 9300: 0.6930244876211664\n",
      "Cost after iteration 9400: 0.6930244876211554\n",
      "Cost after iteration 9500: 0.6930244876211444\n",
      "Cost after iteration 9600: 0.6930244876211333\n",
      "Cost after iteration 9700: 0.6930244876211225\n",
      "Cost after iteration 9800: 0.6930244876211112\n",
      "Cost after iteration 9900: 0.6930244876211002\n",
      "Cost after iteration 9999: 0.6930244876210893\n"
     ]
    }
   ],
   "source": [
    "parameters, costs = L_layer_model(X, Y, layers_dims, num_iterations = 10000, print_cost = True, ss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Y, parameters, ss):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (0 / 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    A, cache = L_model_forward(X, parameters, ss)\n",
    "    \n",
    "    incorrect = (A > 0.5).astype(int) == Y\n",
    "    sum = 0\n",
    "    \n",
    "    for i in range (incorrect.shape[1]):\n",
    "        sum = sum + incorrect[0][i]\n",
    "    \n",
    "    return sum/Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5078322385042952\n"
     ]
    }
   ],
   "source": [
    "p = predict(X,Y,parameters, True)\n",
    "print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
